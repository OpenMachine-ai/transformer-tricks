{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9KAn1NGtAX3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from huggingface_hub import hf_hub_download\n",
        "import gc  # garbage collection needed for low RAM footprint\n",
        "\n",
        "# download Mistral-7B from https://huggingface.co/mistralai/Mistral-7B-v0.1\n",
        "hf_hub_download(repo_id='mistralai/Mistral-7B-v0.1', filename='pytorch_model-00001-of-00002.bin', local_dir='.')\n",
        "hf_hub_download(repo_id='mistralai/Mistral-7B-v0.1', filename='pytorch_model-00002-of-00002.bin', local_dir='.')\n",
        "\n",
        "# load model files, use mmap to keep RAM footprint low\n",
        "m1 = torch.load('pytorch_model-00001-of-00002.bin', weights_only=True, mmap=True)\n",
        "m2 = torch.load('pytorch_model-00002-of-00002.bin', weights_only=True, mmap=True)\n",
        "\n",
        "def get_weights(model, layer, name):\n",
        "  \"\"\"returns weight matrix of specific layer and name (such as Q, K, V)\"\"\"\n",
        "  layer_str = 'layers.' + str(layer)\n",
        "  match name:\n",
        "    case 'Q': suffix = layer_str + '.self_attn.q_proj.weight'\n",
        "    case 'K': suffix = layer_str + '.self_attn.k_proj.weight'\n",
        "    case 'V': suffix = layer_str + '.self_attn.v_proj.weight'\n",
        "    case 'P': suffix = layer_str + '.self_attn.o_proj.weight'\n",
        "    case 'O': suffix = layer_str + '.mlp.down_proj.weight'\n",
        "    case 'E': suffix = 'embed_tokens.weight'\n",
        "  W = model['model.' + suffix].to(torch.float64).numpy()  # convert to float64\n",
        "  return W if name == 'E' else W.T  # transpose weights, except for 'E'\n",
        "\n",
        "for layer in range(0, 32):\n",
        "  print('layer', layer)\n",
        "\n",
        "  # get weights Q, K, V, P, O\n",
        "  model = m1 if layer < 23 else m2  # use m1 for layers 0 to 22\n",
        "  Q = get_weights(model, layer, 'Q')\n",
        "  K = get_weights(model, layer, 'K')\n",
        "  V = get_weights(model, layer, 'V')\n",
        "  P = get_weights(model, layer, 'P')\n",
        "  O = get_weights(model, layer - 1, 'E' if layer == 0 else 'O') # use embedding for 1st layer\n",
        "\n",
        "  # check if weight elimination is numerically identical\n",
        "  Q_inv = np.linalg.inv(Q)  # errors out if matrix is not invertible\n",
        "  K_star = Q_inv @ K\n",
        "  V_star = Q_inv @ V\n",
        "  O_star = O @ Q\n",
        "  print('   is O* @ K* close to O @ K ?  ', np.allclose(O_star @ K_star, O @ K))\n",
        "  print('   is O* @ V* close to O @ V ?  ', np.allclose(O_star @ V_star, O @ V))\n",
        "\n",
        "  # also check if P is invertible\n",
        "  P_inv = np.linalg.inv(P)  # errors out if matrix is not invertible\n",
        "\n",
        "# garbage collection (to avoid colab's RAM limit)\n",
        "del m1, m2, model, Q, K, V, P, O, Q_inv, P_inv, K_star, V_star, O_star\n",
        "gc.collect()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}