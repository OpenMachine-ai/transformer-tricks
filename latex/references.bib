% How to add arXiv references:
%  1) Go to https://www.bibtex.com/c/arxiv-to-bibtex-converter/
%  2) manually add the url to the title field, add the note field,
%     and you can delete the "abstract" field

% Note: we don't use biblatex here, because using biblatex is tricky
% when uploading to arXiv:
% https://github.com/plk/biblatex/wiki/biblatex-and-the-arXiv

% TODO: we could simplify adding the href in the title-fields below,
% but this might require biblatex:
% https://tex.stackexchange.com/questions/23832/biblatex-make-title-hyperlink-to-doi-url-if-available

%----------------------------------------------------------------------
% arXiv
%----------------------------------------------------------------------
@article{vanilla,
  title = {\href{https://arxiv.org/abs/1706.03762}{Attention is all you need}},
  author        = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
                   Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and
                   Kaiser, Lukasz and Polosukhin, Illia",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1706.03762",
  note          = {\textit{arXiv:1706.03762}}
}

@article{RoPE,
  title = {\href{https://arxiv.org/abs/2104.09864}
          {{RoFormer}: Enhanced transformer with Rotary Position Embedding}},
  author        = "Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha,
                   Ahmed and Wen, Bo and Liu, Yunfeng",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2104.09864",
  note          = {\textit{arXiv:2104.09864}}
}

@article{pre-norm,
  title = {\href{https://arxiv.org/abs/2002.04745}
          {On Layer Normalization in the Transformer Architecture}},
  author        = "Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai
                   and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and
                   Lan, Yanyan and Wang, Liwei and Liu, Tie-Yan",
  month         =  feb,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2002.04745",
  note          = {\textit{arXiv:2002.04745}}
}

@article{MQA,
  title = {\href{https://arxiv.org/abs/1911.02150}
          {Fast Transformer Decoding: One Write-Head is All You Need}},
  author        = "Shazeer, Noam",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "1911.02150",
  note          = {\textit{arXiv:1911.02150}}
}

@article{GQA,
  title = {\href{https://arxiv.org/abs/2305.13245}
          {{GQA}: Training generalized multi-query transformer models from multi-head checkpoints}},
  author        = "Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel
                   and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai,
                   Sumit",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.13245",
  note          = {\textit{arXiv:2305.13245}}
}

@article{GLU,
  title = {\href{https://arxiv.org/abs/2002.05202}
          {{GLU} Variants Improve Transformer}},
  author        = "Shazeer, Noam",
  month         =  feb,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2002.05202",
  note          = {\textit{arXiv:2002.05202}}
}

@article{linear,
  title = {\href{https://arxiv.org/abs/2006.16236}
          {Transformers are {RNNs}: Fast autoregressive transformers with linear attention}},
  author        = "Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos
                   and Fleuret, Fran{\c c}ois",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2006.16236",
  note          = "\textit{arXiv:2006.16236}. And \href{https://linear-transformers.com/}{code}"
}

@article{longformer,
  title = {\href{https://arxiv.org/abs/2004.05150}
          {Longformer: The {Long-Document} Transformer}},
  author        = "Beltagy, Iz and Peters, Matthew E and Cohan, Arman",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2004.05150",
  note          = {\textit{arXiv:2004.05150}}
}

@article{flash-attention,
  title = {\href{https://arxiv.org/abs/2205.14135}
          {{FlashAttention}: Fast and memory-efficient exact attention with {IO-awareness}}},
  author        = "Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri
                   and R{\'e}, Christopher",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2205.14135",
  note          = {\textit{arXiv:2205.14135}}
}

@article{quantizable,
  title = {\href{https://arxiv.org/abs/2306.12929}
          {Quantizable transformers: Removing outliers by helping attention heads do nothing}},
  author        = "Bondarenko, Yelysei and Nagel, Markus and Blankevoort,
                   Tijmen",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2306.12929",
  note          = {\textit{arXiv:2306.12929}}
}

@article{simplified,
  title = {\href{https://arxiv.org/abs/2311.01906}{Simplifying Transformer Blocks}},
  author        = "He, Bobby and Hofmann, Thomas",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2311.01906",
  note          = {\textit{arXiv:2311.01906}}
}

@article{skipless,
  title = {\href{https://arxiv.org/abs/2302.10322}
          {Deep transformers without shortcuts: Modifying self-attention for faithful signal propagation}},
  author        = "He, Bobby and Martens, James and Zhang, Guodong and Botev,
                   Aleksandar and Brock, Andrew and Smith, Samuel L and Teh,
                   Yee Whye",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2302.10322",
  note          = "\textit{arXiv:2302.10322}. And \href{https://openreview.net/pdf?id=NPrsUQgMjKK}{ICLR 2023}"
}

@article{PaLM,
  title = {\href{https://arxiv.org/abs/2204.02311}
          {{PaLM}: Scaling language modeling with Pathways}},
  author        = "Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob
                   and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and
                   Barham, Paul and Chung, Hyung Won and Sutton, Charles and
                   Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and
                   Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek
                   and Barnes, Parker and Tay, Yi and Shazeer, Noam and others",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2204.02311",
  note          = {\textit{arXiv:2204.02311}}
}

@article{LLaMA,
  title = {\href{https://arxiv.org/abs/2302.13971}
          {{LLaMA}: Open and efficient foundation language models}},
  author        = "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and
                   Martinet, Xavier and Lachaux, Marie-Anne and Lacroix,
                   Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and
                   Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and
                   Joulin, Armand and Grave, Edouard and Lample, Guillaume",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2302.13971",
  note          = {\textit{arXiv:2302.13971}}
}

@article{Llama2,
  title = {\href{https://arxiv.org/abs/2307.09288}
          {Llama 2: Open foundation and fine-tuned chat models}},
  author        = "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert,
                   Peter and Almahairi, Amjad and Babaei, Yasmine and
                   Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal
                   and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and
                   Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem
                   and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu,
                   Wenyin and Fuller, Brian and Gao, Cynthia and Goswami,
                   Vedanuj and Goyal, Naman and Hartshorn, Anthony and
                   Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas,
                   Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann,
                   Isabel and Korenev, Artem and Koura, Punit Singh and
                   Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and
                   Liskovich, Diana and Lu, Yinghai and Mao, Yuning and
                   Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and
                   Molybog, Igor and Nie, Yixin and Poulton, Andrew and
                   Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and
                   Schelten, Alan and Silva, Ruan and Smith, Eric Michael and
                   Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh
                   and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang
                   and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang,
                   Yuchen and Fan, Angela and Kambadur, Melanie and Narang,
                   Sharan and Rodriguez, Aurelien and Stojnic, Robert and
                   Edunov, Sergey and Scialom, Thomas",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2307.09288",
  note          = {\textit{arXiv:2307.09288}}
}

@article{Mistral,
  title = {\href{https://arxiv.org/abs/2310.06825}{Mistral {7B}}},
  author        = "Jiang, Albert Q and Sablayrolles, Alexandre and Mensch,
                   Arthur and Bamford, Chris and Chaplot, Devendra Singh and
                   Casas, Diego de las and Bressand, Florian and Lengyel,
                   Gianna and Lample, Guillaume and Saulnier, Lucile and
                   Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock,
                   Pierre and Scao, Teven Le and Lavril, Thibaut and Wang,
                   Thomas and Lacroix, Timoth{\'e}e and Sayed, William El",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.06825",
  note          = {\textit{arXiv:2310.06825}}
}

@article{Mixtral,
  title = {\href{https://arxiv.org/abs/2401.04088}{Mixtral of Experts}},
  author        = "Jiang, Albert Q and Sablayrolles, Alexandre and Roux,
                   Antoine and Mensch, Arthur and Savary, Blanche and Bamford,
                   Chris and Chaplot, Devendra Singh and Casas, Diego de las
                   and Hanna, Emma Bou and Bressand, Florian and Lengyel,
                   Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud,
                   L{\'e}lio Renard and Saulnier, Lucile and Lachaux,
                   Marie-Anne and Stock, Pierre and Subramanian, Sandeep and
                   Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and
                   Gervet, Th{\'e}ophile and Lavril, Thibaut and Wang, Thomas
                   and Lacroix, Timoth{\'e}e and Sayed, William El",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2401.04088",
  note          = {\textit{arXiv:2401.04088}}
}

@article{Pythia,
  title = {\href{https://arxiv.org/abs/2304.01373}
          {Pythia: A suite for analyzing large language models across training and scaling}},
  author        = "Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin
                   and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and
                   Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth,
                   Usvsn Sai and Raff, Edward and Skowron, Aviya and Sutawika,
                   Lintang and van der Wal, Oskar",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2304.01373",
  note          = {\textit{arXiv:2304.01373}}
}

@article{whisper,
  title = {\href{https://arxiv.org/abs/2212.04356}
          {Robust speech recognition via large-scale weak supervision}},
  author        = "Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman,
                   Greg and McLeavey, Christine and Sutskever, Ilya",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "eess.AS",
  eprint        = "2212.04356",
  note          = {\textit{arXiv:2212.04356}}
}

@article{micro-paper,
  title = {\href{https://arxiv.org/abs/2302.12854}
          {The {Micro-Paper}: Towards cheaper, citable research ideas and conversations}},
  author        = "Elavsky, Frank",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DL",
  eprint        = "2302.12854",
  note          = {\textit{arXiv:2302.12854}}
}

@article{precompute,
  title = {\href{https://arxiv.org/abs/2402.13388}
          {Transformer tricks: Precomputing the first layer}},
  author        = "Graef, Nils",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2402.13388",
  note          = {\textit{arXiv:2402.13388}}
}

@article{griffin,
  title = {\href{https://arxiv.org/abs/2402.19427}
          {Griffin: Mixing gated linear recurrences with local attention for efficient language models}},
  author        = "De, Soham and Smith, Samuel L and Fernando, Anushan and
                   Botev, Aleksandar and Cristian-Muraru, George and Gu, Albert
                   and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and
                   Srinivasan, Srivatsan and Desjardins, Guillaume and Doucet,
                   Arnaud and Budden, David and Teh, Yee Whye and Pascanu,
                   Razvan and De Freitas, Nando and Gulcehre, Caglar",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2402.19427",
  note          = {\textit{arXiv:2402.19427}}
}

@article{rms,
  title = {\href{https://arxiv.org/abs/1910.07467}
          {Root mean square layer normalization}},
  author        = "Zhang, Biao and Sennrich, Rico",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1910.07467",
  note          = {\textit{arXiv:1910.07467}}
}

@article{openelm,
  title = {\href{https://arxiv.org/abs/2404.14619}
          {{OpenELM}: An efficient language model family with open-source training and inference framework}},
  author        = "Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao,
                   Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan
                   and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry
                   and Zatloukal, Peter and Rastegari, Mohammad",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2404.14619",
  note          = {\textit{arXiv:2404.14619}}
}

@article{QKnorm,
  title = {\href{https://arxiv.org/abs/2010.04245}
          {Query-key normalization for transformers}},
  author        = "Henry, Alex and Dachapally, Prudhvi Raj and Pawar, Shubham
                   and Chen, Yuxuan",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.04245",
  note          = {\textit{arXiv:2010.04245}}
}

@article{layerNorm,
  title = {\href{https://arxiv.org/abs/1607.06450}
          {Layer Normalization}},
  author        = "Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E",
  month         =  jul,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1607.06450",
  note          = {\textit{arXiv:1607.06450}}
}

@article{remove,
  title = {\href{https://arxiv.org/abs/2404.12362}
          {Transformer tricks: Removing weights for skipless transformers}},
  author        = "Graef, Nils",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2404.12362",
  note          = {\textit{arXiv:2404.12362}}
}

@article{FIRE,
  title = {\href{https://arxiv.org/abs/2310.04418}
          {Functional interpolation for relative positions improves long context Transformers}},
  author        = "Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie,
                   Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai,
                   Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli,
                   Srinadh",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2310.04418",
  note          = {\textit{arXiv:2310.04418}}
}

@article{T5,
  title = {\href{https://arxiv.org/abs/1910.10683}
          {Exploring the limits of transfer learning with a unified text-to-text transformer}},
  author        = "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee,
                   Katherine and Narang, Sharan and Matena, Michael and Zhou,
                   Yanqi and Li, Wei and Liu, Peter J",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1910.10683",
  note          = {\textit{arXiv:1910.10683}}
}

@article{code-llama,
  title = {\href{https://arxiv.org/abs/2308.12950}
          {Code Llama: Open foundation models for code}},
  author        = "Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle,
                   Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing
                   Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain
                   and Remez, Tal and Rapin, J{\'e}r{\'e}my and Kozhevnikov,
                   Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt,
                   Manish and Ferrer, Cristian Canton and Grattafiori, Aaron
                   and Xiong, Wenhan and D{\'e}fossez, Alexandre and Copet,
                   Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis
                   and Usunier, Nicolas and Scialom, Thomas and Synnaeve,
                   Gabriel",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2308.12950",
  note          = {\textit{arXiv:2308.12950}}
}

@article{codeGemma,
  title = {\href{https://arxiv.org/abs/2406.11409}
          {{CodeGemma}: Open Code Models Based on Gemma}},
  author        = "{CodeGemma Team} and Zhao, Heri and Hui, Jeffrey and
                   Howland, Joshua and Nguyen, Nam and Zuo, Siqi and Hu, Andrea
                   and Choquette-Choo, Christopher A and Shen, Jingyue and
                   Kelley, Joe and Bansal, Kshitij and Vilnis, Luke and Wirth,
                   Mateo and Michel, Paul and Choy, Peter and Joshi, Pratik and
                   Kumar, Ravin and Hashmi, Sarmad and Agrawal, Shubham and
                   Gong, Zhitao and Fine, Jane and Warkentin, Tris and Hartman,
                   Ale Jakse and Ni, Bin and Korevec, Kathy and Schaefer, Kelly
                   and Huffman, Scott",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2406.11409",
  note          = {\textit{arXiv:2406.11409}}
}

@articl{aya,
  title = {\href{https://arxiv.org/abs/2405.15032}
          {Aya 23: Open weight releases to further multilingual progress}},
  author        = "Aryabumi, Viraat and Dang, John and Talupuru, Dwarak and
                   Dash, Saurabh and Cairuz, David and Lin, Hangyu and
                   Venkitesh, Bharat and Smith, Madeline and Campos, Jon Ander
                   and Tan, Yi Chern and Marchisio, Kelly and Bartolo, Max and
                   Ruder, Sebastian and Locatelli, Acyr and Kreutzer, Julia and
                   Frosst, Nick and Gomez, Aidan and Blunsom, Phil and Fadaee,
                   Marzieh and {\"U}st{\"u}n, Ahmet and Hooker, Sara",
  month         =  may,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2405.15032",
  note          = {\textit{arXiv:2405.15032}}
}

@article{phi3,
  title = {\href{https://arxiv.org/abs/2404.14219}
          {Phi-3 technical report: A highly capable language model locally on your phone}},
  author        = "Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and
                   Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and
                   Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl,
                   Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck,
                   Johan and Bubeck, S{\'e}bastien and Cai, Martin and Cai, Qin
                   and Chaudhary, Vishrav and Chen, Dong and Chen, Dongdong and
                   Chen, Weizhu and Chen, Yen-Chun and Chen, Yi-Ling and Cheng,
                   Hao and Chopra, Parul and Dai, Xiyang and Dixon, Matthew and
                   Eldan, Ronen and Fragoso, Victor and Gao, Jianfeng and Gao,
                   Mei and Gao, Min and Garg, Amit and Del Giorno, Allie and
                   Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman
                   and Hao, Junheng and Hewett, Russell J and Hu, Wenxiang and
                   Huynh, Jamie and Iter, Dan and Jacobs, Sam Ade and
                   Javaheripi, Mojan and Jin, Xin and Karampatziakis, Nikos and
                   Kauffmann, Piero and Khademi, Mahoud and Kim, Dongwoo and
                   Kim, Young Jin and Kurilenko, Lev and Lee, James R and Lee,
                   Yin Tat and Li, Yuanzhi and Li, Yunsheng and Liang, Chen and
                   Liden, Lars and Lin, Xihui and Lin, Zeqi and Liu, Ce and
                   Liu, Liyuan and Liu, Mengchen and Liu, Weishung and Liu,
                   Xiaodong and Luo, Chong and Madan, Piyush and Mahmoudzadeh,
                   Ali and Majercak, David and Mazzola, Matt and Mendes, Caio
                   C{\'e}sar Teodoro and Mitra, Arindam and Modi, Hardik and
                   Nguyen, Anh and Norick, Brandon and Patra, Barun and
                   Perez-Becker, Daniel and Portet, Thomas and Pryzant, Reid
                   and Qin, Heyang and Radmilac, Marko and Ren, Liliang and de
                   Rosa, Gustavo and Rosset, Corby and Roy, Sambudha and
                   Ruwase, Olatunji and Saarikivi, Olli and Saied, Amin and
                   Salim, Adil and Santacroce, Michael and Shah, Shital and
                   Shang, Ning and Sharma, Hiteshi and Shen, Yelong and Shukla,
                   Swadheen and Song, Xia and Tanaka, Masahiro and Tupini,
                   Andrea and Vaddamanu, Praneetha and Wang, Chunyu and Wang,
                   Guanhua and Wang, Lijuan and Wang, Shuohang and Wang, Xin
                   and Wang, Yu and Ward, Rachel and Wen, Wen and Witte,
                   Philipp and Wu, Haiping and Wu, Xiaoxia and Wyatt, Michael
                   and Xiao, Bin and Xu, Can and Xu, Jiahang and Xu, Weijian
                   and Xue, Jilong and Yadav, Sonali and Yang, Fan and Yang,
                   Jianwei and Yang, Yifan and Yang, Ziyi and Yu, Donghan and
                   Yuan, Lu and Zhang, Chenruidong and Zhang, Cyril and Zhang,
                   Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang, Yue and
                   Zhang, Yunan and Zhou, Xiren",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2404.14219",
  note          = {\textit{arXiv:2404.14219}}
}

@article{bitnet,
  title = {\href{https://arxiv.org/abs/2402.17764}
          {The Era of 1-bit {LLMs}: All Large Language Models are in 1.58 Bits}},
  author        = "Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei
                   and Wang, Wenhui and Huang, Shaohan and Dong, Li and Wang,
                   Ruiping and Xue, Jilong and Wei, Furu",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2402.17764",
  note          = {\textit{arXiv:2402.17764}}
}

@article{olmo,
  title = {\href{https://arxiv.org/abs/2402.00838}
          {{OLMo}: Accelerating the science of language models}},
  author        = "Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia,
                   Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha,
                   Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang,
                   Yizhong and Arora, Shane and Atkinson, David and Authur,
                   Russell and Chandu, Khyathi Raghavi and Cohan, Arman and
                   Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel,
                   Jack and Khot, Tushar and Merrill, William and Morrison,
                   Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam,
                   Crystal and Peters, Matthew E and Pyatkin, Valentina and
                   Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh
                   and Smith, Will and Strubell, Emma and Subramani, Nishant
                   and Wortsman, Mitchell and Dasigi, Pradeep and Lambert,
                   Nathan and Richardson, Kyle and Zettlemoyer, Luke and Dodge,
                   Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A and
                   Hajishirzi, Hannaneh",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2402.00838",
  note          = {\textit{arXiv:2402.00838}}
}

@article{cronos,
  title = {\href{https://arxiv.org/abs/2403.07815}
          {Chronos: Learning the language of time series}},
  author        = "Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner
                   and Zhang, Xiyuan and Mercado, Pedro and Shen, Huibin and
                   Shchur, Oleksandr and Rangapuram, Syama Sundar and Arango,
                   Sebastian Pineda and Kapoor, Shubham and Zschiegner, Jasper
                   and Maddix, Danielle C and Wang, Hao and Mahoney, Michael W
                   and Torkkola, Kari and Wilson, Andrew Gordon and
                   Bohlke-Schneider, Michael and Wang, Yuyang",
  month         =  mar,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2403.07815",
  note          = {\textit{arXiv:2403.07815}}
}

@article{qwen2-audio,
  title = {\href{https://arxiv.org/abs/2407.10759}
          {{Qwen2-Audio} Technical Report}},
  author        = "Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and
                   Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv,
                   Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang
                   and Zhou, Jingren",
  month         =  jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "eess.AS",
  eprint        = "2407.10759",
  note          = {\textit{arXiv:2407.10759}}
}

@article{flan,
  title = {\href{https://arxiv.org/abs/2210.11416}
          {Scaling instruction-finetuned language models}},
  author        = "Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph,
                   Barret and Tay, Yi and Fedus, William and Li, Yunxuan and
                   Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha
                   and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun
                   and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha
                   and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin
                   and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and
                   Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai,
                   Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H and
                   Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou,
                   Denny and Le, Quoc V and Wei, Jason",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2210.11416",
  note          = {\textit{arXiv:2210.11416}}
}

@article{llava,
  title = {\href{https://arxiv.org/abs/2310.03744}
          {Improved baselines with visual instruction tuning}},
  author        = "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong
                   Jae",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2310.03744",
  note          = {\textit{arXiv:2310.03744}}
}

@article{VL-cache,
  title = {\href{https://arxiv.org/abs/2410.23317}
          {{VL-cache}: Sparsity and modality-aware {KV} cache compression for vision-language model inference acceleration}},
  author        = "Tu, Dezhan and Vashchilenko, Danylo and Lu, Yuzhe and Xu,
                   Panpan",
  month         =  oct,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2410.23317",
  note          = {\textit{arXiv:2410.23317}}
}

@article{pagedAttn,
  title = {\href{https://arxiv.org/abs/2309.06180}
          {Efficient memory management for large language model serving with {PagedAttention}}},
  author        = "Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng,
                   Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez,
                   Joseph E and Zhang, Hao and Stoica, Ion",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2309.06180",
  note          = {\textit{arXiv:2309.06180}}
}

%----------------------------------------------------------------------
% wikipedia
%----------------------------------------------------------------------
@misc{chebyshev,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Chebyshev_polynomials}
           {Chebyshev polynomials}},
  year   = "2024",
  note   = "Accessed Feb-2024"
}

@misc{minimax,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Minimax_approximation_algorithm}
           {Minimax approximation algorithm}},
  year   = "2024",
  note   = "Accessed Feb-2024"
 }

@misc{multinomial,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Multinomial_theorem}
           {Multinomial theorem}},
  year   = "2024",
  note   = "Accessed Feb-2024"
}

@misc{invertible,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Invertible_matrix}
           {Invertible matrix}},
  year   = "2024",
  note   = "Accessed Mar-2024"
}

@misc{ReLU,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Rectifier_(neural_networks)}
           {Rectifier (neural networks)}},
  year   = "2024",
  note   = "Accessed June-2024"
}

@misc{apple-wiki,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Apple_silicon}
           {Apple silicon}},
  year   = "2025",
  note   = "Accessed Jan-2025"
}

@misc{TPU-wiki,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Tensor_Processing_Unit}
           {Tensor Processing Unit}},
  year   = "2025",
  note   = "Accessed Jan-2025"
}

@misc{nvidia-wiki,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Nvidia_DGX#Accelerators}
           {Nvidia DGX}},
  year   = "2025",
  note   = "Accessed Jan-2025"
}

%----------------------------------------------------------------------
% misc.
%----------------------------------------------------------------------
@article{parallel,
  title  = {\href{https://github.com/kingoflolz/mesh-transformer-jax}
           {{GPT-J-6B}: A 6 billion parameter autoregressive language model}},
  author = "Wang, Ben and Komatsuzaki, Aran",
  year   =  2021,
  note   = {\textit{Github repo}}
}

@article{gemma,
  title  = {\href{https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf}
           {Gemma: Open Models Based on Gemini Research and Technology}},
  author = "{Gemma Team, Google DeepMind}",
  year   =  2024,
}

@article{MoE,
  title  = {\href{https://huggingface.co/blog/moe}{Mixture of Experts Explained}},
  author = "Sanseviero, Omar and Tunstall, Lewis and Schmid, Philipp and
            Mangrulkar, Sourab and Belkada, Younes and Cuenca, Pedro",
  month  =  dec,
  year   =  2023,
  note   = {\textit{HuggingFace blog}}
}

@article{exp.c,
  title  = {\href{https://netlib.org/fdlibm/e_exp.c}{exp.c library}},
  author = "Sun Microsystems",
  year   =  2004,
  url    = "https://netlib.org/fdlibm/e_exp.c"
}

@article{tricks,
  title  = {\href{https://github.com/OpenMachine-ai/transformer-tricks}
           {Transformer tricks}},
  author = "OpenMachine",
  year   =  2024,
  note   = {\textit{Github repo}}
}

@article{hfFlashNorm,
  title  = {\href{https://huggingface.co/open-machine/FlashNorm}
           {FlashNorm}},
  author = "OpenMachine",
  year   =  2024,
  note   = {\textit{HuggingFace repo}}
}

@misc{smollm,
  title  = {\href{https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B}
           {SmolLM2 - with great data, comes great performance}},
  author = {Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Martín Blázquez and Lewis Tunstall and Agustín Piqueres and Andres Marafioti and Cyril Zakka and Leandro von Werra and Thomas Wolf},
  year   = {2024},
  note   = {\textit{HuggingFace repo}}
}

@article{smolvlm,
  title  = {\href{https://huggingface.co/blog/smolvlm}
           {SmolVLM - small yet mighty Vision Language Model}},
  author = "Andres Marafioti and Merve Noyan and Miquel Farré and Elie Bakouch and Pedro Cuenca",
  month  =  nov,
  year   =  2024,
  note   = {\textit{HuggingFace blog}}
}

@article{gpt2,
  title  = {\href{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
           {Language Models are Unsupervised Multitask Learners}},
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year   = {2019}
}

@article{llava-next,
  title  = {\href{https://llava-vl.github.io/blog/2024-04-30-llava-next-video/}
           {LLaVA-NeXT: A Strong Zero-shot Video Understanding Model}},
  author = {Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},
  month  = {April},
  year={2024}
}

@article{vicuna,
  title  = {\href{https://lmsys.org/blog/2023-03-30-vicuna/}
           {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality}},
  author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
  month  = {March},
  year   = {2023}
}

@article{slim-podcast,
  title  = {\href{https://notebooklm.google.com/notebook/ac47a53c-866b-4271-ab79-bc48d1b41722/audio}
           {Podcast about Slim Attention}},
  author = "{Generated by Notebook LM}",
  month  = {Jan},
  year   = {2025}
}

@article{llamafile,
  title  = {\href{https://github.com/Mozilla-Ocho/llamafile}
           {llamafile}},
  author = "Mozilla",
  note   = {\textit{Github repo}}
}

@article{vLLM,
  title  = {\href{https://github.com/vllm-project/vllm}
           {vLLM}},
  author = "vLLM Project",
  note   = {\textit{Github repo}}
}

@article{ollama,
  title  = {\href{https://github.com/ollama/ollama}
           {Ollama}},
  note   = {\textit{Github repo}}
}
