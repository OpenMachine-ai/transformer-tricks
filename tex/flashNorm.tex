% To generate PDF, type ./run flashNorm.tex

\documentclass{article}

\usepackage[preprint, nonatbib]{neurips_2025}
%\usepackage[nonatbib]{neurips_2025}         % for submission
%\usepackage[final, nonatbib]{neurips_2025}  % final version
\usepackage{neurips_2025_mods}  % my mods for neurips_2025.sty

% shortcuts
\newcommand{\mat}[1]{\mathbf{#1}}     % shortcut for matrix
\newcommand{\RMS}[1]{\text{RMS}(#1)}  % shortcut for RMS(x)
\def\rms{\text{RMS}(\vec{a})}         % RMS(a)
\def\f1n{\frac{1}{n}}                 % 1/n
\def\sas{\sum_{i=1}^n a_i^2}           % sum over a_i squared
\def\W*{\mat{W}^\ast}                 % matrix W*
\def\V*{\mat{V}^\ast}                 % matrix V*
\def\mW{\mat{W}}                      % matrix W
\def\mV{\mat{V}}                      % matrix V
\def\a{\vec{a}}                       % vector a
\def\b{\vec{b}}                       % vector b
\def\c{\vec{c}}                       % vector c
\def\vb{\vec{\beta}}                  % vector beta
\def\vx{\vec{x}}                      % vector x
\def\vy{\vec{y}}                      % vector y
\def\vz{\vec{z}}                      % vector z
\def\vg{\vec{g}}                      % vector g
\def\vs{\vec{s}}                      % vector s
\def\cosi{\cos{(\cdot)}}              % cos(.)
\def\sini{\sin{(\cdot)}}              % sin(.)

\title{FlashNorm: fast normalization for LLMs}
%\title{Flash normalization: fast normalization for LLMs}
%\title{Flash normalization: fast RMSNorm for LLMs}

\author{Nils Graef\thanks{\texttt{info@openmachine.ai}}, \, Andrew Wasielewski, \, Matthew Clapp \\
  \href{https://openmachine.ai}{OpenMachine}}

\begin{document} \maketitle

\begin{abstract}
This paper presents FlashNorm, which is an exact but faster implementation of RMSNorm followed by linear layers. RMSNorm \citep{rms} is used by many LLMs such as Llama, Gemma, Mistral, and OLMo 2 \citep{LLaMA, gemma, mistral, olmo2}. FlashNorm also speeds up Layer Normalization \citep{layerNorm} and its recently proposed replacement Dynamic Tanh (DyT) \citep{DyT}. FlashNorm reduces the number of parameter tensors by simply merging the normalization weights with the weights of the next linear layer. Watch our explainer video \citep{flashNorm-video} and see \citep{slimAttn, tricks, remove, precompute} for code and more transformer tricks.
\end{abstract}

\section{Flash normalization}
\begin{figure}[h!] \centering  % the [h!] tries to place the picture right here
  \includegraphics[scale=1.0]{../doc/fig/flashNorm_fig1.pdf}
  \caption{Mathematically identical implementations of RMSNorm followed by a linear layer: (a) unoptimized version with weight matrix $\mat{W}$; (b) optimized version with normalization weights $g_i$ merged into the linear layer with new weights $\W*$; (c) optimized version with deferred normalization. The $\triangleq$ symbol denotes mathematical identity.}
\label{fig1} \end{figure}

RMSNorm \citep{rms} normalizes the elements  $a_i$ of vector $\a$ as $y_i = \frac{a_i}{\rms} \cdot g_i$ with $\rms = \sqrt{\f1n \sas}$ and normalization weights $g_i$. In transformer \citep{vanilla} and other neural networks, RMSNorm is often followed by a linear layer as illustrated in Fig. \ref{fig1}(a), which we optimize as follows:
\begin{itemize}[topsep=-1pt]
  \item \textbf{Weightless normalization (aka non-parametric normalization)}: We merge the normalization weights $g_i$ into the linear layer with weights $\mat{W}$, resulting in a modified weight matrix $\W*$ with $W_{i,j}^\ast = g_i \cdot W_{i,j}$ as illustrated in Fig. \ref{fig1}(b). This works for linear layers with and without bias.
  \item \textbf{Deferred normalization}: Instead of normalizing before the linear layer, we normalize after the linear layer, as shown in Fig. \ref{fig1}(c). This only works if the linear layer is bias-free, which is the case for many LLMs such as Llama, Gemma, Mistral, OLMo and OpenELM. Specifically, the output of the linear layer in Fig. \ref{fig1}(b) is $\vz = \left( \a \cdot \frac{1}{\rms} \right) \W*$, which is identical to $\vz = \left( \a \, \W* \right) \cdot \frac{1}{\rms}$ because matrix multiplication by a scalar is commutative. If the linear layer has a bias at its output, then the normalization (i.e. scaling by $\frac{1}{\rms}$) must be done before adding the bias.
\end{itemize}

In summary, FlashNorm eliminates the normalization weights and defers the normalization to the output of the linear layer, which removes a compute bottleneck described at the end of this paper. Deferring the normalization is similar to Flash Attention \citep{flash-attention}, where the normalization by the softmax denominator is done after the multiplication of softmax arguments with value projections (V) (so that keys and values can be processed in \emph{parallel}). Therefore, we call our implementation \emph{flash} normalization (or FlashNorm), which allows us to compute the linear layer and $\rms$ in \emph{parallel} (instead of sequentially).

\citeauthor{openelm} report significant changes in the overall tokens-per-second throughput when they modify the layer normalization implementation, which they attribute to a lack of kernel fusion for the underlying GPU. The simplifications presented here reduce the number of operations and thus the number of the individual kernel launches mentioned in \citep{openelm}.

\subsection{Support for normalization bias and DyT bias}
Layer normalization (LayerNorm) \citep{layerNorm} and DyT \citep{DyT} can have a bias vector $\vb$ right after scaling by weights $g_i$. Figure \ref{figA} illustrates how the bias vector $\vb$ can be moved to the output of the linear layer and then be added to the bias vector $\c$ of the linear layer, resulting in the new bias term $\c^{\, \ast} = \c + \vb \, \mW$, see Fig. \ref{figA}(b). After this elimination of $\vb$, the normalization weights $g_i$ can be merged into the linear layer as described in the previous section and illustrated in Fig. \ref{fig1}(b).

\begin{figure}[h!] \centering  % the [h!] tries to place the picture right here
  \includegraphics[scale=0.9]{../doc/fig/flashNorm_figA.pdf}
  \caption{Elimination of bias vector $\vb$: (a) Before elimination with $\vb$ between normalization weights $\vg$ and linear layer. (b) Optimized version with new bias term  $\c^{\, \ast} = \c + \vb \, \mW$ at the output.}
\label{figA} \end{figure}

\subsection{Merging mean centering into a preceding linear layer}
Note that LayerNorm consists of mean centering followed by RMSNorm. If the mean centering is preceded by a linear layer with weight matrix $\mV$, then we can eliminate the entire mean centering by modifying the weight matrix as explained in this section. Fig. \ref{figB}(a) shows the weight matrix $\mV$ followed by the mean centering, which is followed by RMSNorm.

\begin{figure}[h!] \centering  % the [h!] tries to place the picture right here
  \includegraphics[scale=0.9]{../doc/fig/flashNorm_figB.pdf}
  \caption{Elimination of mean centering: (a) Original weight matrix $\mV$ followed by mean centering. (b) Optimized version where the mean centering is merged into the modified weight matrix $\V*$.}
\label{figB} \end{figure}

The mean $\mu$ is calculated from the linear layer outputs $y_j$ as $\mu = \frac{1}{n} \sum_{j=1}^n y_j$. Note that $\vy = \vx \, \mV$, i.e. $y_j = \sum_{i=1}^n x_i v_{i, j}$ where $v_{i, j}$ are the weights of matrix $\mV$. Plugging the last equation into the $\mu$ expression lets us calculate $\mu$ directly from the input $\vx$ as
\begin{equation*}
  \mu = \frac{1}{n} \sum_{j=1}^n \sum_{i=1}^n x_i v_{i, j} = \frac{1}{n} \sum_{i=1}^n x_i \left[ \sum_{j=1}^n v_{i, j} \right] = \frac{1}{n} \sum_{i=1}^n x_i s_i
\end{equation*}
where we define vector $\vs$ with $s_i = \sum_{j=1}^n v_{i, j}$ the sum of row $i$ of weight matrix $\mV$. In other words, $\mu$ is the inner-product of vectors $\vx$ and $\vs$ divided by $n$. The outputs $a_j$ of the mean centering are
\begin{equation*}
  \a_j = y_j - \mu = \sum_{i=1}^n x_i v_{i, j} - \mu = \sum_{i=1}^n x_i v_{i, j} - \frac{1}{n} \sum_{i=1}^n x_i s_i = \sum_{i=1}^n x_i \left( v_{i, j} - \frac{1}{n} s_i \right)
                   = \sum_{i=1}^n x_i v^{\, \ast}_{i, j}
\end{equation*}
From the last identity follows that the new weights $v^{\, \ast}_{i, j}$ of matrix $\V*$ of Fig. \ref{figB}(b) are computed as $v^{\, \ast}_{i, j} = v_{i, j} - \frac{1}{n} s_i$. This trick can be used to retrofit existing LayerNorm models with RMSNorm without any retraining.

\section{Flash normalization for FFN}
For the feed-forward networks (FFN) of LLMs, the linear layers at the FFN input usually have more output channels than input channels. In this case, deferring the normalization requires more scaling operations (i.e. more multiplications). This section details ways to reduce the number of scaling operations for bias-free FFNs.

\subsection{Flash normalization for FFNs with ReLU}
\begin{figure}[h!] \centering
  \includegraphics[scale=1.0]{../doc/fig/flashNorm_fig2.pdf}
  \caption{FFN with ReLU and preceding flash normalization: (a) unoptimized version; (b) optimized version where the normalization is deferred to the output of the FFN. Up and Down denote the linear layers for up and down projections.}
\label{fig2} \end{figure}

Even though ReLU is a nonlinear function, multiplying its argument by a non-negative scaling factor $s$ is the same as scaling its output by $s$, i.e. $\text{ReLU}(s \cdot \a) = s \cdot \text{ReLU}(\a)$ for $s \ge 0$ \citep{ReLU}. Because of this scale-invariance, we can defer the normalization to the output of the FFN as illustrated in Fig. \ref{fig2}(b), which saves $f - n$ multipliers.

\subsection{Flash normalization for FFNs with GLU variant}
Fig. \ref{fig3}(a) shows an FFN with a GLU variant \citep{GLU} and flash normalization at its input. The flash normalization requires two sets of $f$ multipliers at the outputs of the Gate and Up linear layers in Fig. \ref{fig3}(a). One set can be deferred to the FFN output in Fig. \ref{fig3}(b), which saves $f - n$ multipliers.
\begin{figure}[h!] \centering
  \includegraphics[scale=0.9]{../doc/fig/flashNorm_fig3.pdf}
  \caption{FFN with GLU variant and preceding flash normalization: (a) unoptimized version; (b) optimized version with fewer scaling multipliers. Gate, Up, and Down denote the linear layers for gate, up, and down projections.}
\label{fig3} \end{figure}

\textbf{Special case for ReGLU and Bilinear GLU}: If the activation function is ReLU (aka ReGLU \citep{GLU}) or just linear (aka bilinear GLU \citep{GLU}), then we can also eliminate the scaling before the activation function and combine it with the scaling at the output as illustrated in Fig. \ref{fig4}(b), which saves $2f - n$ multipliers. Now the output scaling is using the reciprocal of the squared RMS as scaling value, which is the same as the reciprocal of the mean-square (MS):
\begin{equation*}
  \frac{1}{(\rms)^2} = \frac{1}{\text{MS}(\a)}
  = \frac{1}{\f1n \sas} = \frac{n}{\sas}
\end{equation*}

\begin{figure}[h!] \centering
  \includegraphics[scale=0.9]{../doc/fig/flashNorm_fig4.pdf}
  \caption{FFN with ReGLU (or bilinear GLU) and preceding flash normalization: (a) unoptimized version; (b) optimized version with fewer scaling multipliers.}
\label{fig4} \end{figure}

\section{Flash normalization for attention with RoPE}
Fig. \ref{fig5}(a) shows the Q and K linear layers with flash normalization followed by RoPE \citep{RoPE} and scaled dot-product attention \citep{vanilla}. More details on Figure \ref{fig5}:
\begin{itemize}[topsep=-1pt]
  \item Q* and K* are the linear layers for Q (queries) and K (keys) fused with the normalization weights of the activation vector $\a$ (according to flash normalization).
  \item $h$ is the dimension of the attention heads.
  \item The boxes labeled cos, sin, and RoPE perform $\vy = \vx \cdot \cosi + \text{permute}(\vx) \cdot \sini$, where
  \begin{itemize}[topsep=-1pt]
    \item $\text{permute}(\vx) = (-x_2, x_1, -x_4, x_3, \dots, -x_h, x_{h-1})$, see equation (34) of \citep{RoPE} for more details.
    \item $\cosi = (\cos m \theta_1, \cos m \theta_1, \cos m \theta_2, \cos m \theta_2, \dots, \cos m \theta_{h/2}, \cos m \theta_{h/2})$ for position $m$.
    \item $\sini = (\sin m \theta_1, \sin m \theta_1, \sin m \theta_2, \sin m \theta_2, \dots, \sin m \theta_{h/2}, \sin m \theta_{h/2})$ for position $m$.
  \end{itemize}
  \item Note that $\cosi$ and $\sini$ only depend on the position of activation vector $\a$ and are shared among all attention heads. Therefore, it’s more efficient to first scale $\cosi$ and $\sini$ by $1/ \rms$ as illustrated in Fig. \ref{fig5}(b). This saves $2hH - h$ multipliers, where $H$ is the number of attention heads.
  \item Furthermore, we can fuse the scaling factor $1/ \sqrt{h}$ of the scaled dot-product with the $1/ \rms$ factor (note that we need to use $\sqrt{1/ \sqrt{h}}$ as a scaling factor for this).
  \item Unfortunately, the V linear layer (value projection) still needs the normalization at its output.
\end{itemize}
\begin{figure}[h!] \centering
  \includegraphics[scale=0.9]{../doc/fig/flashNorm_fig5.pdf}
  \caption{Flash normalization for scaled dot-product attention with RoPE: (a) unoptimized version; (b) optimized version where the normalization is fused with $\cosi$ and $\sini$.}
\label{fig5} \end{figure}

\section{Optimizations for QK-normalization with RoPE}
Gemma 3 \citep{gemma3}, OLMo 2 \citep{olmo2}, OpenELM \citep{openelm} and other LLMs use query-key normalization (QK-norm) \citep{QKnorm}. For example, each layer of Gemma 3 and OpenELM has the following two sets of normalization weights:
\begin{itemize}[topsep=-1pt]
  \item \verb+q_norm_weight+: query normalization weights for all heads of this layer
  \item \verb+k_norm_weight+: key normalization weights for all heads of this layer
\end{itemize}
Unfortunately, FlashNorm can't be applied to QK-norm. But for the type of QK-norm used in Gemma 3 and OpenELM, we can apply the following two optimizations detailed in the next sections:
\begin{enumerate}[topsep=-1pt]
  \item Eliminate the RMS calculation before the Q and K linear layers.
  \item Fuse the normalization weights with RoPE.
\end{enumerate}

\subsection{Eliminate RMS calculation before QK linear layers}
Fig. \ref{fig6}(a) shows a linear layer with flash normalization followed by an additional normalization. The weights of the first normalization are already merged into the linear layer weights $\W*$. Note that $\RMS{s \cdot \a} = s \cdot \rms$ where $s$ is scalar and $\a$ is a vector. Due to this scale-invariance of the RMS function, the second multiplier (scaler $s_c$) in the pipeline of Fig. \ref{fig6}(a) cancels out the first multiplier (scaler $s_a$). Fig. \ref{fig6}(b) takes advantage of this property. We can express this by using the vectors $\a, \b, \c$ along the datapath in Fig. \ref{fig6} as follows:
\begin{itemize}[topsep=-1pt]
  \item Note that $s_c = \frac{1}{\RMS{\c}} = \frac{1}{\RMS{\b \cdot s_a}} = \frac{1}{s_a \cdot \RMS{\b}} = \frac{s_b}{s_a}$.
  \item With above, we can show that the $y$ outputs of figures \ref{fig6}(a) and \ref{fig6}(b) are identical:
    \begin{equation*}
      y = \a \cdot \W* \cdot s_a \cdot s_c \cdot \vg = \a \cdot \W* \cdot s_a \cdot \frac{s_b}{s_a} \cdot \vg
      = \a \cdot \W* \cdot s_b \cdot \vg
    \end{equation*}
\end{itemize}

\begin{figure}[h!] \centering
  \includegraphics[scale=0.9]{../doc/fig/flashNorm_fig6.pdf}
  \caption{Linear layer with flash normalization followed by a second normalization: (a) unoptimized version; (b) optimized version.}
\label{fig6} \end{figure}

The scale-invariance property of $\rms$ doesn’t hold exactly true for RMS with epsilon (see appendix). This should not matter because the epsilon only makes an impact if the RMS (or energy) of the activation vector is very small, in which case the epsilon limits the up-scaling of this low-energy activation vector.

\begin{figure}[h!] \centering
  \includegraphics[scale=0.9]{../doc/fig/flashNorm_fig7.pdf}
  \caption{QK-norm with RoPE: (a) unoptimized version; (b) optimized version.}
\label{fig7} \end{figure}

\subsection{Fuse normalization weights with RoPE}
Fig. \ref{fig7}(a) illustrates QK-norm with RoPE. If the QK-norm weights are the same for all heads of a layer, as is the case for Gemma 3 and  OpenELM \citep{gemma3, openelm}, then we can fuse them with RoPE's $\cosi$ and $\sini$ as follows: multiply $\cosi$ and $\sini$ with the normalization weights and then share the fused $\cosi$ and $\sini$ vectors across all heads of the LLM layer as shown in Fig. \ref{fig7}(b). This requires permutation of the normalization weights $\vg$ so that the boxes labeled cos, sin, and RoPE in Fig. \ref{fig7}(b) perform $\vy = \vx \cdot \left( \cosi \cdot \vg \right) + \text{permute}(\vx) \cdot \left( \sini \cdot \text{permuteg}(\vg) \right)$, where $\text{permuteg}(\vg) = (g_2, g_1, g_4, g_3, \dots, g_h, g_{h-1})$. For simplicity, Fig. \ref{fig7}(b) doesn't show the permutation of the normalization weights.

\section{Bottleneck of RMS normalization for batch 1}
This section describes the compute bottleneck of RMS normalization that exists for batch size 1. This bottleneck is different from the bottleneck detailed in \citep{openelm}. Let’s consider a processor with one vector unit and one matrix unit:
\begin{itemize}[topsep=-1pt]
  \item The matrix multiplications of the linear layers are performed by the matrix unit, while the vector unit performs vector-wise operations such as RMSNorm and FlashNorm.
  \item Let’s assume that the vector unit can perform $m$ operations per cycle and the matrix unit can perform $m^2$ operations per cycle, where $m$ is the processor width. Specifically:
  \begin{itemize}[topsep=-1pt]
    \item Multiplying an $n$-element vector with an $n \times n$ matrix takes $n^2$ MAD (multiply-add) operations, which takes $n^2/m^2$ cycles with our matrix unit.
    \item Calculating $1/\rms$ takes $n$ MAD operations (for squaring and adding) plus 2 scalar operations (for $\sqrt{n/x}$), which takes $n/m$ cycles with our vector unit if we ignore the 2 scalar operations.
    \item Scaling an $n$-element vector by a scaling factor takes $n$ multiply operations, which takes $n/m$ cycles.
  \end{itemize}
\end{itemize}

For the example $n = 512, m = 128$ and batch 1, Fig. \ref{fig8} shows timing diagrams without and with deferred normalization:
\begin{itemize}[topsep=-1pt]
  \item Without deferred normalization, the matrix unit has to wait for 8 cycles until the vector unit has calculated the RMS value and completed the scaling by $1/ \rms$ as illustrated in Fig. \ref{fig8}(a).
  \item As shown in Fig. \ref{fig8}(b), it is possible to start the matrix unit 3 cycles earlier if the weight matrix $\mat{W}$ is processed in row-major order for example. But the RMS calculation still presents a bottleneck.
  \item FlashNorm eliminates this bottleneck: With deferred normalization, the matrix unit computes the vector-matrix multiplication in parallel to the vector unit's RMS calculation as shown in Fig. \ref{fig8}(c). The scaling at the end can be performed in parallel to the matrix unit if $\mat{W}$ is processed in column-major order for example.
\end{itemize}

\begin{figure}[h!] \centering
  \includegraphics[scale=1.0]{../doc/fig/flashNorm_fig8.pdf}
  \caption{Timing diagrams for $n = 512, m = 128$: (a) without deferred normalization; (b) with interleaved scaling and vector-matrix multiplication; (c) with deferred normalization.}
\label{fig8} \end{figure}

\section{Experiments and conclusions}
Refer to \citep{hfFlashNorm, tricks} for Python code that demonstrates the mathematical equivalency of the optimizations presented in this paper. The overall speedup of FlashNorm is modest: We measured a throughput of 204 tokens per second for OpenELM-270M with 4-bit weight quantization using the MLX framework on an M1 MacBook Air. This throughput increases to only 225 tokens per second when we remove RMSNorm entirely. Therefore, the maximum possible speedup of any RMSNorm optimization is $\leq$ 10\% for this model.

For many applications, the main advantage of FlashNorm is simplification. This is similar to the simplifications we get from using RMSNorm over Layer Normalization (LayerNorm \citep{layerNorm}), and from PaLM's removal of bias-parameters from all linear layers  \citep{PaLM}.

Future work includes integrating FlashNorm into popular frameworks such as HuggingFace Transformers \citep{HFtransformers}, whisper.cpp \citep{whisper-cpp}, llama.cpp \citep{llama-cpp}, vLLM \citep{vLLM}, llamafile \citep{llamafile}, LM Studio \citep{lmstudio}, Ollama \citep{ollama}, SGLang \citep{sglang}, and combining it with parameter quantization.

\section*{Acknowledgments}
We would like to thank Dmitry Belenko for helpful feedback on this work.

\appendix

\section{RMS with epsilon}
Many implementations add a small epsilon $\epsilon$ to the RMS value to limit the resulting scaling factor $1/\rms$ and to avoid division by zero as follows:
\begin{equation*}
 \text{RMSe}(\a) = \sqrt{\epsilon + \f1n \sas} = \sqrt{\epsilon + \left( \rms \right)^2}
\end{equation*}

$\text{RMSe}(\a)$ can be used as a drop-in-replacement for RMS. The popular HuggingFace transformer library calls this epsilon \verb+rms_norm_eps+, which is set to $10^{-5}$ for Llama3.

\section{Eliminating $1/n$}
This section details a small optimization that eliminates the constant term $1/n$ from the RMS calculation. First, we factor out $1/n$ as follows:
\begin{equation*}
  \rms = \sqrt{\f1n \sas} = \sqrt{\f1n} \sqrt{\sas} = \sqrt{\f1n} \cdot \text{RSS}(\a)
\end{equation*}
where $\text{RSS}(\a) = \sqrt{\sas}$. We can now merge the constant term into the normalization weights $g_i$ as follows:
\begin{equation*}
  y_i = \frac{a_i}{\rms} \cdot g_i =
  \frac{a_i}{\text{RSS}(\a)} \sqrt{n} \cdot g_i =
  \frac{a_i}{\text{RSS}(\a)}          \cdot g_i^\ast
\end{equation*}
with new normalization weights $g_i^\ast = \sqrt{n} \cdot g_i$ . These new normalization weights can now be merged with the weights $\mat{W}$ of the following linear layer as shown in the previous sections. This optimization also applies for the case where we add an epsilon as detailed in the previous section. In this case, we factor out $1/n$ as follows:
\begin{equation*}
  \text{RMSe}(\a) = \sqrt{\epsilon + \f1n \sas}
  = \sqrt{\f1n \left( n \epsilon + \sas \right)}
  %= \sqrt{\f1n} \sqrt{n \epsilon + \sas}
  = \sqrt{\f1n} \cdot \text{RSSe}(\a)
\end{equation*}
where $\text{RSSe}(\a) = \sqrt{n \epsilon + \sas}$.

\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}
