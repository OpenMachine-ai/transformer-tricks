% How to add arXiv references:
%  1) Go to https://www.bibtex.com/c/arxiv-to-bibtex-converter/
%  2) manually add the url to the title field, add the note field,
%     and you can delete the "abstract" field

% Note: we don't use biblatex here, because using biblatex is tricky
% when uploading to arXiv:
% https://github.com/plk/biblatex/wiki/biblatex-and-the-arXiv

% TODO: we could simplify adding the href in the title-fields below,
% but this might require biblatex:
% https://tex.stackexchange.com/questions/23832/biblatex-make-title-hyperlink-to-doi-url-if-available

%----------------------------------------------------------------------
% arXiv
%----------------------------------------------------------------------
@article{vanilla,
  title = {\href{https://arxiv.org/abs/1706.03762}{Attention is all you need}},
  author        = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
                   Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and
                   Kaiser, Lukasz and Polosukhin, Illia",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1706.03762",
  note          = {\textit{arXiv:1706.03762}}
}

@article{RoPE,
  title = {\href{https://arxiv.org/abs/2104.09864}
          {{RoFormer}: Enhanced transformer with Rotary Position Embedding}},
  author        = "Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha,
                   Ahmed and Wen, Bo and Liu, Yunfeng",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2104.09864",
  note          = {\textit{arXiv:2104.09864}}
}

@article{pre-norm,
  title = {\href{https://arxiv.org/abs/2002.04745}
          {On Layer Normalization in the Transformer Architecture}},
  author        = "Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai
                   and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and
                   Lan, Yanyan and Wang, Liwei and Liu, Tie-Yan",
  month         =  feb,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2002.04745",
  note          = {\textit{arXiv:2002.04745}}
}

@article{MQA,
  title = {\href{https://arxiv.org/abs/1911.02150}
          {Fast Transformer Decoding: One Write-Head is All You Need}},
  author        = "Shazeer, Noam",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "1911.02150",
  note          = {\textit{arXiv:1911.02150}}
}

@article{GQA,
  title = {\href{https://arxiv.org/abs/2305.13245}
          {{GQA}: Training generalized multi-query transformer models from multi-head checkpoints}},
  author        = "Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel
                   and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai,
                   Sumit",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.13245",
  note          = {\textit{arXiv:2305.13245}}
}

@article{GLU,
  title = {\href{https://arxiv.org/abs/2002.05202}
          {{GLU} Variants Improve Transformer}},
  author        = "Shazeer, Noam",
  month         =  feb,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2002.05202",
  note          = {\textit{arXiv:2002.05202}}
}

@article{linear,
  title = {\href{https://arxiv.org/abs/2006.16236}
          {Transformers are {RNNs}: Fast autoregressive transformers with linear attention}},
  author        = "Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos
                   and Fleuret, Fran{\c c}ois",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2006.16236",
  note          = "\textit{arXiv:2006.16236}. And \href{https://linear-transformers.com/}{code}"
}

@article{longformer,
  title = {\href{https://arxiv.org/abs/2004.05150}
          {Longformer: The {Long-Document} Transformer}},
  author        = "Beltagy, Iz and Peters, Matthew E and Cohan, Arman",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2004.05150",
  note          = {\textit{arXiv:2004.05150}}
}

@article{flash-attention,
  title = {\href{https://arxiv.org/abs/2205.14135}
          {{FlashAttention}: Fast and memory-efficient exact attention with {IO-awareness}}},
  author        = "Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri
                   and R{\'e}, Christopher",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2205.14135",
  note          = {\textit{arXiv:2205.14135}}
}

@article{quantizable,
  title = {\href{https://arxiv.org/abs/2306.12929}
          {Quantizable transformers: Removing outliers by helping attention heads do nothing}},
  author        = "Bondarenko, Yelysei and Nagel, Markus and Blankevoort,
                   Tijmen",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2306.12929",
  note          = {\textit{arXiv:2306.12929}}
}

@article{simplified,
  title = {\href{https://arxiv.org/abs/2311.01906}{Simplifying Transformer Blocks}},
  author        = "He, Bobby and Hofmann, Thomas",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2311.01906",
  note          = {\textit{arXiv:2311.01906}}
}

@article{skipless,
  title = {\href{https://arxiv.org/abs/2302.10322}
          {Deep transformers without shortcuts: Modifying self-attention for faithful signal propagation}},
  author        = "He, Bobby and Martens, James and Zhang, Guodong and Botev,
                   Aleksandar and Brock, Andrew and Smith, Samuel L and Teh,
                   Yee Whye",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2302.10322",
  note          = "\textit{arXiv:2302.10322}. And \href{https://openreview.net/pdf?id=NPrsUQgMjKK}{ICLR 2023}"
}

@article{PaLM,
  title = {\href{https://arxiv.org/abs/2204.02311}
          {{PaLM}: Scaling language modeling with Pathways}},
  author        = "Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob
                   and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and
                   Barham, Paul and Chung, Hyung Won and Sutton, Charles and
                   Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and
                   Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek
                   and Barnes, Parker and Tay, Yi and Shazeer, Noam and others",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2204.02311",
  note          = {\textit{arXiv:2204.02311}}
}

@article{LLaMA,
  title = {\href{https://arxiv.org/abs/2302.13971}
          {{LLaMA}: Open and efficient foundation language models}},
  author        = "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and
                   Martinet, Xavier and Lachaux, Marie-Anne and Lacroix,
                   Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and
                   Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and
                   Joulin, Armand and Grave, Edouard and Lample, Guillaume",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2302.13971",
  note          = {\textit{arXiv:2302.13971}}
}

@article{Llama2,
  title = {\href{https://arxiv.org/abs/2307.09288}
          {Llama 2: Open foundation and fine-tuned chat models}},
  author        = "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert,
                   Peter and Almahairi, Amjad and Babaei, Yasmine and
                   Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal
                   and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and
                   Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem
                   and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu,
                   Wenyin and Fuller, Brian and Gao, Cynthia and Goswami,
                   Vedanuj and Goyal, Naman and Hartshorn, Anthony and
                   Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas,
                   Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann,
                   Isabel and Korenev, Artem and Koura, Punit Singh and
                   Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and
                   Liskovich, Diana and Lu, Yinghai and Mao, Yuning and
                   Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and
                   Molybog, Igor and Nie, Yixin and Poulton, Andrew and
                   Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and
                   Schelten, Alan and Silva, Ruan and Smith, Eric Michael and
                   Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh
                   and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang
                   and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang,
                   Yuchen and Fan, Angela and Kambadur, Melanie and Narang,
                   Sharan and Rodriguez, Aurelien and Stojnic, Robert and
                   Edunov, Sergey and Scialom, Thomas",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2307.09288",
  note          = {\textit{arXiv:2307.09288}}
}

@article{Mistral,
  title = {\href{https://arxiv.org/abs/2310.06825}{Mistral {7B}}},
  author        = "Jiang, Albert Q and Sablayrolles, Alexandre and Mensch,
                   Arthur and Bamford, Chris and Chaplot, Devendra Singh and
                   Casas, Diego de las and Bressand, Florian and Lengyel,
                   Gianna and Lample, Guillaume and Saulnier, Lucile and
                   Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock,
                   Pierre and Scao, Teven Le and Lavril, Thibaut and Wang,
                   Thomas and Lacroix, Timoth{\'e}e and Sayed, William El",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.06825",
  note          = {\textit{arXiv:2310.06825}}
}

@article{Mixtral,
  title = {\href{https://arxiv.org/abs/2401.04088}{Mixtral of Experts}},
  author        = "Jiang, Albert Q and Sablayrolles, Alexandre and Roux,
                   Antoine and Mensch, Arthur and Savary, Blanche and Bamford,
                   Chris and Chaplot, Devendra Singh and Casas, Diego de las
                   and Hanna, Emma Bou and Bressand, Florian and Lengyel,
                   Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud,
                   L{\'e}lio Renard and Saulnier, Lucile and Lachaux,
                   Marie-Anne and Stock, Pierre and Subramanian, Sandeep and
                   Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and
                   Gervet, Th{\'e}ophile and Lavril, Thibaut and Wang, Thomas
                   and Lacroix, Timoth{\'e}e and Sayed, William El",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2401.04088",
  note          = {\textit{arXiv:2401.04088}}
}

@article{Pythia,
  title = {\href{https://arxiv.org/abs/2304.01373}
          {Pythia: A suite for analyzing large language models across training and scaling}},
  author        = "Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin
                   and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and
                   Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth,
                   Usvsn Sai and Raff, Edward and Skowron, Aviya and Sutawika,
                   Lintang and van der Wal, Oskar",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2304.01373",
  note          = {\textit{arXiv:2304.01373}}
}

@article{whisper,
  title = {\href{https://arxiv.org/abs/2212.04356}
          {Robust speech recognition via large-scale weak supervision}},
  author        = "Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman,
                   Greg and McLeavey, Christine and Sutskever, Ilya",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "eess.AS",
  eprint        = "2212.04356",
  note          = {\textit{arXiv:2212.04356}}
}

@article{micro-paper,
  title = {\href{https://arxiv.org/abs/2302.12854}
          {The {Micro-Paper}: Towards cheaper, citable research ideas and conversations}},
  author        = "Elavsky, Frank",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DL",
  eprint        = "2302.12854",
  note          = {\textit{arXiv:2302.12854}}
}

@article{precompute,
  title = {\href{https://arxiv.org/abs/2402.13388}
          {Transformer tricks: Precomputing the first layer}},
  author        = "Graef, Nils",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2402.13388",
  note          = {\textit{arXiv:2402.13388}}
}

@article{griffin,
  title = {\href{https://arxiv.org/abs/2402.19427}
          {Griffin: Mixing gated linear recurrences with local attention for efficient language models}},
  author        = "De, Soham and Smith, Samuel L and Fernando, Anushan and
                   Botev, Aleksandar and Cristian-Muraru, George and Gu, Albert
                   and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and
                   Srinivasan, Srivatsan and Desjardins, Guillaume and Doucet,
                   Arnaud and Budden, David and Teh, Yee Whye and Pascanu,
                   Razvan and De Freitas, Nando and Gulcehre, Caglar",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2402.19427",
  note          = {\textit{arXiv:2402.19427}}
}

@article{rms,
  title = {\href{https://arxiv.org/abs/1910.07467}
          {Root mean square layer normalization}},
  author        = "Zhang, Biao and Sennrich, Rico",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1910.07467",
  note          = {\textit{arXiv:1910.07467}}
}

@article{openelm,
  title = {\href{https://arxiv.org/abs/2404.14619}
          {{OpenELM}: An efficient language model family with open-source training and inference framework}},
  author        = "Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao,
                   Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan
                   and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry
                   and Zatloukal, Peter and Rastegari, Mohammad",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2404.14619",
  note          = {\textit{arXiv:2404.14619}}
}

@article{QKnorm,
  title = {\href{https://arxiv.org/abs/2010.04245}
          {Query-key normalization for transformers}},
  author        = "Henry, Alex and Dachapally, Prudhvi Raj and Pawar, Shubham
                   and Chen, Yuxuan",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.04245",
  note          = {\textit{arXiv:2010.04245}}
}

@article{layerNorm,
  title = {\href{https://arxiv.org/abs/1607.06450}
          {Layer Normalization}},
  author        = "Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E",
  month         =  jul,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1607.06450",
  note          = {\textit{arXiv:1607.06450}}
}

@article{remove,
  title = {\href{https://arxiv.org/abs/2404.12362}
          {Transformer tricks: Removing weights for skipless transformers}},
  author        = "Graef, Nils",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2404.12362",
  note          = {\textit{arXiv:2404.12362}}
}

@article{FIRE,
  title = {\href{https://arxiv.org/abs/2310.04418}
          {Functional interpolation for relative positions improves long context Transformers}},
  author        = "Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie,
                   Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai,
                   Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli,
                   Srinadh",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2310.04418",
  note          = {\textit{arXiv:2310.04418}}
}

@article{T5,
  title = {\href{https://arxiv.org/abs/1910.10683}
          {Exploring the limits of transfer learning with a unified text-to-text transformer}},
  author        = "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee,
                   Katherine and Narang, Sharan and Matena, Michael and Zhou,
                   Yanqi and Li, Wei and Liu, Peter J",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1910.10683",
  note          = {\textit{arXiv:1910.10683}}
}

@article{code-llama,
  title = {\href{https://arxiv.org/abs/2308.12950}
          {Code Llama: Open foundation models for code}},
  author        = "Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle,
                   Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing
                   Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain
                   and Remez, Tal and Rapin, J{\'e}r{\'e}my and Kozhevnikov,
                   Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt,
                   Manish and Ferrer, Cristian Canton and Grattafiori, Aaron
                   and Xiong, Wenhan and D{\'e}fossez, Alexandre and Copet,
                   Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis
                   and Usunier, Nicolas and Scialom, Thomas and Synnaeve,
                   Gabriel",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2308.12950",
  note          = {\textit{arXiv:2308.12950}}
}

@article{codeGemma,
  title = {\href{https://arxiv.org/abs/2406.11409}
          {{CodeGemma}: Open Code Models Based on Gemma}},
  author        = "{CodeGemma Team} and Zhao, Heri and Hui, Jeffrey and
                   Howland, Joshua and Nguyen, Nam and Zuo, Siqi and Hu, Andrea
                   and Choquette-Choo, Christopher A and Shen, Jingyue and
                   Kelley, Joe and Bansal, Kshitij and Vilnis, Luke and Wirth,
                   Mateo and Michel, Paul and Choy, Peter and Joshi, Pratik and
                   Kumar, Ravin and Hashmi, Sarmad and Agrawal, Shubham and
                   Gong, Zhitao and Fine, Jane and Warkentin, Tris and Hartman,
                   Ale Jakse and Ni, Bin and Korevec, Kathy and Schaefer, Kelly
                   and Huffman, Scott",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2406.11409",
  note          = {\textit{arXiv:2406.11409}}
}

@articl{aya,
  title = {\href{https://arxiv.org/abs/2405.15032}
          {Aya 23: Open weight releases to further multilingual progress}},
  author        = "Aryabumi, Viraat and Dang, John and Talupuru, Dwarak and
                   Dash, Saurabh and Cairuz, David and Lin, Hangyu and
                   Venkitesh, Bharat and Smith, Madeline and Campos, Jon Ander
                   and Tan, Yi Chern and Marchisio, Kelly and Bartolo, Max and
                   Ruder, Sebastian and Locatelli, Acyr and Kreutzer, Julia and
                   Frosst, Nick and Gomez, Aidan and Blunsom, Phil and Fadaee,
                   Marzieh and {\"U}st{\"u}n, Ahmet and Hooker, Sara",
  month         =  may,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2405.15032",
  note          = {\textit{arXiv:2405.15032}}
}

@article{phi3,
  title = {\href{https://arxiv.org/abs/2404.14219}
          {Phi-3 technical report: A highly capable language model locally on your phone}},
  author        = "Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and
                   Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and
                   Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl,
                   Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck,
                   Johan and Bubeck, S{\'e}bastien and Cai, Martin and Cai, Qin
                   and Chaudhary, Vishrav and Chen, Dong and Chen, Dongdong and
                   Chen, Weizhu and Chen, Yen-Chun and Chen, Yi-Ling and Cheng,
                   Hao and Chopra, Parul and Dai, Xiyang and Dixon, Matthew and
                   Eldan, Ronen and Fragoso, Victor and Gao, Jianfeng and Gao,
                   Mei and Gao, Min and others",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2404.14219",
  note          = {\textit{arXiv:2404.14219}}
}

@article{bitnet,
  title = {\href{https://arxiv.org/abs/2402.17764}
          {The Era of 1-bit {LLMs}: All Large Language Models are in 1.58 Bits}},
  author        = "Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei
                   and Wang, Wenhui and Huang, Shaohan and Dong, Li and Wang,
                   Ruiping and Xue, Jilong and Wei, Furu",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2402.17764",
  note          = {\textit{arXiv:2402.17764}}
}

@article{olmo,
  title = {\href{https://arxiv.org/abs/2402.00838}
          {{OLMo}: Accelerating the science of language models}},
  author        = "Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia,
                   Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha,
                   Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang,
                   Yizhong and Arora, Shane and Atkinson, David and Authur,
                   Russell and Chandu, Khyathi Raghavi and Cohan, Arman and
                   Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel,
                   Jack and Khot, Tushar and Merrill, William and Morrison,
                   Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam,
                   Crystal and Peters, Matthew E and Pyatkin, Valentina and
                   Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh
                   and Smith, Will and Strubell, Emma and Subramani, Nishant
                   and Wortsman, Mitchell and Dasigi, Pradeep and Lambert,
                   Nathan and Richardson, Kyle and Zettlemoyer, Luke and Dodge,
                   Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A and
                   Hajishirzi, Hannaneh",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2402.00838",
  note          = {\textit{arXiv:2402.00838}}
}

@article{olmo2,
  title = {\href{https://arxiv.org/abs/2501.00656}
          {2 {OLMo} 2 Furious}},
  author        = "OLMo, Team and Walsh, Pete and Soldaini, Luca and
                   Groeneveld, Dirk and Lo, Kyle and Arora, Shane and Bhagia,
                   Akshita and Gu, Yuling and Huang, Shengyi and Jordan, Matt
                   and Lambert, Nathan and Schwenk, Dustin and Tafjord, Oyvind
                   and Anderson, Taira and Atkinson, David and Brahman, Faeze
                   and Clark, Christopher and Dasigi, Pradeep and Dziri, Nouha
                   and Guerquin, Michal and Ivison, Hamish and Koh, Pang Wei
                   and Liu, Jiacheng and Malik, Saumya and Merrill, William and
                   Miranda, Lester James V and Morrison, Jacob and Murray,
                   Tyler and Nam, Crystal and Pyatkin, Valentina and Rangapur,
                   Aman and Schmitz, Michael and Skjonsberg, Sam and Wadden,
                   David and Wilhelm, Christopher and Wilson, Michael and
                   Zettlemoyer, Luke and Farhadi, Ali and Smith, Noah A and
                   Hajishirzi, Hannaneh",
  month         =  dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2501.00656",
  note          = {\textit{arXiv:2501.00656}}
}

@article{gemma3,
  title = {\href{https://arxiv.org/abs/2503.19786}
          {Gemma 3 Technical Report}},
  author        = "{Gemma Team} and Kamath, Aishwarya and Ferret, Johan and
                   Pathak, Shreya and Vieillard, Nino and Merhej, Ramona and
                   Perrin, Sarah and Matejovicova, Tatiana and Ram{\'e},
                   Alexandre and Rivi{\`e}re, Morgane and Rouillard, Louis and
                   Mesnard, Thomas and Cideron, Geoffrey and Grill,
                   Jean-Bastien and Ramos, Sabela and Yvinec, Edouard and
                   Casbon, Michelle and Pot, Etienne and Penchev, Ivo and Liu,
                   Ga{\"e}l and Visin, Francesco and Kenealy, Kathleen and
                   Beyer, Lucas and Zhai, Xiaohai and Tsitsulin, Anton and
                   Busa-Fekete, Robert and Feng, Alex and Sachdeva, Noveen and
                   Coleman, Benjamin and Gao, Yi and Mustafa, Basil and Barr,
                   Iain and Parisotto, Emilio and Tian, David and Eyal, Matan
                   and Cherry, Colin and Peter, Jan-Thorsten and Sinopalnikov,
                   Danila and others",
  month         =  mar,
  year          =  2025,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2503.19786",
  note          = {\textit{arXiv:2503.19786}}
}

@article{chronos,
  title = {\href{https://arxiv.org/abs/2403.07815}
          {Chronos: Learning the language of time series}},
  author        = "Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner
                   and Zhang, Xiyuan and Mercado, Pedro and Shen, Huibin and
                   Shchur, Oleksandr and Rangapuram, Syama Sundar and Arango,
                   Sebastian Pineda and Kapoor, Shubham and Zschiegner, Jasper
                   and Maddix, Danielle C and Wang, Hao and Mahoney, Michael W
                   and Torkkola, Kari and Wilson, Andrew Gordon and
                   Bohlke-Schneider, Michael and Wang, Yuyang",
  month         =  mar,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2403.07815",
  note          = {\textit{arXiv:2403.07815}}
}

@article{qwen2-audio,
  title = {\href{https://arxiv.org/abs/2407.10759}
          {{Qwen2-Audio} Technical Report}},
  author        = "Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and
                   Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv,
                   Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang
                   and Zhou, Jingren",
  month         =  jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "eess.AS",
  eprint        = "2407.10759",
  note          = {\textit{arXiv:2407.10759}}
}

@article{flan,
  title = {\href{https://arxiv.org/abs/2210.11416}
          {Scaling instruction-finetuned language models}},
  author        = "Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph,
                   Barret and Tay, Yi and Fedus, William and Li, Yunxuan and
                   Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha
                   and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun
                   and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha
                   and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin
                   and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and
                   Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai,
                   Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H and
                   Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou,
                   Denny and Le, Quoc V and Wei, Jason",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2210.11416",
  note          = {\textit{arXiv:2210.11416}}
}

@article{llava,
  title = {\href{https://arxiv.org/abs/2310.03744}
          {Improved baselines with visual instruction tuning}},
  author        = "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong
                   Jae",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2310.03744",
  note          = {\textit{arXiv:2310.03744}}
}

@article{VL-cache,
  title = {\href{https://arxiv.org/abs/2410.23317}
          {{VL-cache}: Sparsity and modality-aware {KV} cache compression for vision-language model inference acceleration}},
  author        = "Tu, Dezhan and Vashchilenko, Danylo and Lu, Yuzhe and Xu,
                   Panpan",
  month         =  oct,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2410.23317",
  note          = {\textit{arXiv:2410.23317}}
}

@article{pagedAttn,
  title = {\href{https://arxiv.org/abs/2309.06180}
          {Efficient memory management for large language model serving with {PagedAttention}}},
  author        = "Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng,
                   Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez,
                   Joseph E and Zhang, Hao and Stoica, Ion",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2309.06180",
  note          = {\textit{arXiv:2309.06180}}
}

@article{metagene,
  title = {\href{https://arxiv.org/abs/2501.02045}
          {METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring}},
  author = {Ollie Liu and Sami Jaghouar and Johannes Hagemann and Shangshang Wang and Jason Wiemels and Jeff Kaufman and Willie Neiswanger},
  year   = {2025},
  eprint = {arXiv:2501.02045},
  note   = {\textit{arXiv:2501.02045}}
}

@article{moonshine,
  title = {\href{https://arxiv.org/abs/2410.15608}
          {Moonshine: Speech Recognition for Live Transcription and Voice Commands}},
  author = {Nat Jeffries and Evan King and Manjunath Kudlur and Guy Nicholson and James Wang and Pete Warden},
  year   = {2024},
  eprint = {arXiv:2410.15608},
  note   = {\textit{arXiv:2410.15608}}
}

@artile{dclm,
  title = {\href{https://arxiv.org/abs/2406.11794}
          {DataComp-LM: In search of the next generation of training sets for language models}},
  author = {Jeffrey Li and Alex Fang and Georgios Smyrnis and Maor Ivgi and Matt Jordan and Samir Gadre and Hritik Bansal and Etash Guha and Sedrick Keh and Kushal Arora and Saurabh Garg and Rui Xin and Niklas Muennighoff and Reinhard Heckel and Jean Mercat and Mayee Chen and Suchin Gururangan and Mitchell Wortsman and Alon Albalak and Yonatan Bitton and Marianna Nezhurina and Amro Abbas and Cheng-Yu Hsieh and Dhruba Ghosh and Josh Gardner and Maciej Kilian and Hanlin Zhang and Rulin Shao and Sarah Pratt and Sunny Sanyal and Gabriel Ilharco and Giannis Daras and Kalyani Marathe and Aaron Gokaslan and Jieyu Zhang and Khyathi Chandu and Thao Nguyen and Igor Vasiljevic and Sham Kakade and Shuran Song and Sujay Sanghavi and Fartash Faghri and Sewoong Oh and Luke Zettlemoyer and Kyle Lo and Alaaeldin El-Nouby and Hadi Pouransari and Alexander Toshev and Stephanie Wang and Dirk Groeneveld and Luca Soldaini and Pang Wei Koh and Jenia Jitsev and Thomas Kollar and Alexandros G. Dimakis and Yair Carmon and Achal Dave and Ludwig Schmidt and Vaishaal Shankar},
  year   = {2024},
  eprint = {arXiv:2406.11794},
  note   = {\textit{arXiv:2406.11794}}
}

@article{DMC,
  title  = {\href{https://arxiv.org/abs/2403.09636}
           {Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference}},
  author = {Piotr Nawrot and Adrian Łańcucki and Marcin Chochowski and David Tarjan and Edoardo M. Ponti},
  year   = {2024},
  eprint = {arXiv:2403.09636},
  howpublished = {Proceedings of the 41st International Conference on Machine Learning (2024) 37396-37412},
  note   = {\textit{arXiv:2403.09636}}
}

@article{deepseek-v2,
  title  = {\href{https://arxiv.org/abs/2405.04434}
           {DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}},
  author = {DeepSeek-AI and Aixin Liu and Bei Feng and Bin Wang and Bingxuan Wang and Bo Liu and Chenggang Zhao and Chengqi Dengr and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Hanwei Xu and Hao Yang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jin Chen and Jingyang Yuan and Junjie Qiu and others},
  %Junxiao Song and Kai Dong and Kaige Gao and Kang Guan and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruizhe Pan and Runxin Xu and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Size Zheng and T. Wang and Tian Pei and Tian Yuan and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Liu and Xin Xie and Xingkai Yu and Xinnan Song and Xinyi Zhou and Xinyu Yang and Xuan Lu and Xuecheng Su and Y. Wu and Y. K. Li and Y. X. Wei and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Zheng and Yichao Zhang and Yiliang Xiong and Yilong Zhao and Ying He and Ying Tang and Yishi Piao and Yixin Dong and Yixuan Tan and Yiyuan Liu and Yongji Wang and Yongqiang Guo and Yuchen Zhu and Yuduan Wang and Yuheng Zou and Yukun Zha and Yunxian Ma and Yuting Yan and Yuxiang You and Yuxuan Liu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhewen Hao and Zhihong Shao and Zhiniu Wen and Zhipeng Xu and Zhongyu Zhang and Zhuoshu Li and Zihan Wang and Zihui Gu and Zilin Li and Ziwei Xie},
  year = {2024},
  eprint = {arXiv:2405.04434},
  note   = {\textit{arXiv:2405.04434}}
}

@article{olmoe,
  title  = {\href{https://arxiv.org/abs/2409.02060}
           {OLMoE: Open Mixture-of-Experts Language Models}},
  author = {Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A. Smith and Pang Wei Koh and Amanpreet Singh and Hannaneh Hajishirzi},
  year   = {2024},
  eprint = {arXiv:2409.02060},
  note   = {\textit{arXiv:2409.02060}}
}

@article{crisper,
  title  = {\href{https://arxiv.org/abs/2408.16589}
           {CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions}},
  author = {Laurin Wagner and Bernhard Thallinger and Mario Zusag},
  year   = {2024},
  eprint = {arXiv:2408.16589},
  note   = {\textit{arXiv:2408.16589}}
}

@article{stableCode,
  title  = {\href{https://arxiv.org/abs/2404.01226}
           {Stable Code Technical Report}},
  author = {Nikhil Pinnaparaju and Reshinth Adithyan and Duy Phung and Jonathan Tow and James Baicoianu and Ashish Datta and Maksym Zhuravinskyi and Dakota Mahan and Marco Bellagente and Carlos Riquelme and Nathan Cooper},
  year   = {2024},
  eprint = {arXiv:2404.01226},
  note   = {\textit{arXiv:2404.01226}}
}

@article{stableLM,
  title  = {\href{https://arxiv.org/abs/2402.17834}
           {Stable LM 2 1.6B Technical Report}},
  author = {Marco Bellagente and Jonathan Tow and Dakota Mahan and Duy Phung and Maksym Zhuravinskyi and Reshinth Adithyan and James Baicoianu and Ben Brooks and Nathan Cooper and Ashish Datta and Meng Lee and Emad Mostaque and Michael Pieler and Nikhil Pinnaparju and Paulo Rocha and Harry Saini and Hannah Teufel and Niccolo Zanichelli and Carlos Riquelme},
  year   = {2024},
  eprint = {arXiv:2402.17834},
  note   = {\textit{arXiv:2402.17834}}
}

@article{openMath,
  title  = {\href{https://arxiv.org/abs/2402.10176}
           {OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset}},
  author = {Shubham Toshniwal and Ivan Moshkov and Sean Narenthiran and Daria Gitman and Fei Jia and Igor Gitman},
  year   = {2024},
  eprint = {arXiv:2402.10176},
  note   = {\textit{arXiv:2402.10176}}
}

@article{maira2,
  title  = {\href{https://arxiv.org/abs/2406.04449}
           {MAIRA-2: Grounded Radiology Report Generation}},
  author = {Shruthi Bannur and Kenza Bouzid and Daniel C. Castro and Anton Schwaighofer and Anja Thieme and Sam Bond-Taylor and Maximilian Ilse and Fernando Pérez-García and Valentina Salvatelli and Harshita Sharma and Felix Meissen and Mercy Ranjit and Shaury Srivastav and Julia Gong and Noel C. F. Codella and Fabian Falck and Ozan Oktay and Matthew P. Lungren and Maria Teodora Wetscherek and Javier Alvarez-Valle and Stephanie L. Hyland},
  year   = {2024},
  eprint = {arXiv:2406.04449},
  note   = {\textit{arXiv:2406.04449}}
}

@article{timesFM,
  title  = {\href{https://arxiv.org/abs/2310.10688}
           {A decoder-only foundation model for time-series forecasting}},
  author = {Abhimanyu Das and Weihao Kong and Rajat Sen and Yichen Zhou},
  year   = {2023},
  eprint = {arXiv:2310.10688},
  note   = {\textit{arXiv:2310.10688}}
}

@article{metricX,
  title  = {\href{https://arxiv.org/abs/2410.03983}
           {MetricX-24: The Google Submission to the WMT 2024 Metrics Shared Task}},
  author = {Juraj Juraska and Daniel Deutsch and Mara Finkelstein and Markus Freitag},
  year   = {2024},
  eprint = {arXiv:2410.03983},
  note   = {\textit{arXiv:2410.03983}}
}

@article{biomedLM,
  title  = {\href{https://arxiv.org/abs/2403.18421}
           {BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text}},
  author = {Elliot Bolton and Abhinav Venigalla and Michihiro Yasunaga and David Hall and Betty Xiong and Tony Lee and Roxana Daneshjou and Jonathan Frankle and Percy Liang and Michael Carbin and Christopher D. Manning},
  year   = {2024},
  eprint = {arXiv:2403.18421},
  note   = {\textit{arXiv:2403.18421}}
}

@article{miniCPMv2,
  title  = {\href{https://arxiv.org/abs/2408.01800}
           {MiniCPM-V: A GPT-4V Level MLLM on Your Phone}},
  author = {Yuan Yao and Tianyu Yu and Ao Zhang and Chongyi Wang and Junbo Cui and Hongji Zhu and Tianchi Cai and Haoyu Li and Weilin Zhao and Zhihui He and Qianyu Chen and Huarong Zhou and Zhensheng Zou and Haoye Zhang and Shengding Hu and Zhi Zheng and Jie Zhou and Jie Cai and Xu Han and Guoyang Zeng and Dahai Li and Zhiyuan Liu and Maosong Sun},
  year   = {2024},
  eprint = {arXiv:2408.01800},
  note   = {\textit{arXiv:2408.01800}}
}

@article{bloom,
  title  = {\href{https://arxiv.org/abs/2211.05100}
           {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}},
  author = {BigScience Workshop and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ilić and Daniel Hesslow and Roman Castagné and Alexandra Sasha Luccioni and François Yvon and Matthias Gallé and Jonathan Tow and Alexander M. Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Benoît Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Laurençon and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and others},
  year   = {2022},
  eprint = {arXiv:2211.05100},
  note   = {\textit{arXiv:2211.05100}}
}

@article{llmCompiler,
  title  = {\href{https://arxiv.org/abs/2407.02524}
           {Meta Large Language Model Compiler: Foundation Models of Compiler Optimization}},
  author = {Chris Cummins and Volker Seeker and Dejan Grubisic and Baptiste Roziere and Jonas Gehring and Gabriel Synnaeve and Hugh Leather},
  year   = {2024},
  eprint = {arXiv:2407.02524},
  note   = {\textit{arXiv:2407.02524}}
}

@article{drama,
  title  = {\href{https://arxiv.org/abs/2502.18460}
           {DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers}},
  author = {Xueguang Ma and Xi Victoria Lin and Barlas Oguz and Jimmy Lin and Wen-tau Yih and Xilun Chen},
  year   = {2025},
  eprint = {arXiv:2502.18460},
  note   = {\textit{arXiv:2502.18460}}
}

@article{chameleon,
  title  = {\href{https://arxiv.org/abs/2405.09818}
           {Chameleon: Mixed-Modal Early-Fusion Foundation Models}},
  author = {Chameleon Team},
  year   = {2024},
  eprint = {arXiv:2405.09818},
  note   = {\textit{arXiv:2405.09818}}
}

@article{DyT,
  title  = {\href{https://arxiv.org/abs/2503.10622}
           {Transformers without Normalization}},
  author = {Jiachen Zhu and Xinlei Chen and Kaiming He and Yann LeCun and Zhuang Liu},
  year   = {2025},
  eprint = {arXiv:2503.10622},
  note   = {\textit{arXiv:2503.10622}}
}

@article{slimAttn,
  title  = {\href{https://arxiv.org/abs/2503.05840}
           {Slim attention: cut your context memory in half without loss of accuracy -- K-cache is all you need for MHA}},
  author = {Nils Graef and Andrew Wasielewski},
  year   = {2025},
  eprint = {arXiv:2503.05840},
  note   = {\textit{arXiv:2503.05840}}
}

%----------------------------------------------------------------------
% wikipedia
%----------------------------------------------------------------------
@misc{chebyshev,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Chebyshev_polynomials}
           {Chebyshev polynomials}},
  year   = "2024",
  note   = "Accessed Feb-2024"
}

@misc{minimax,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Minimax_approximation_algorithm}
           {Minimax approximation algorithm}},
  year   = "2024",
  note   = "Accessed Feb-2024"
 }

@misc{multinomial,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Multinomial_theorem}
           {Multinomial theorem}},
  year   = "2024",
  note   = "Accessed Feb-2024"
}

@misc{invertible,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Invertible_matrix}
           {Invertible matrix}},
  year   = "2024",
  note   = "Accessed Mar-2024"
}

@misc{ReLU,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Rectifier_(neural_networks)}
           {Rectifier (neural networks)}},
  year   = "2024",
  note   = "Accessed June-2024"
}

@misc{apple-wiki,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Apple_silicon}
           {Apple silicon}},
  year   = "2025",
  note   = "Accessed Jan-2025"
}

@misc{TPU-wiki,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Tensor_Processing_Unit}
           {Tensor Processing Unit}},
  year   = "2025",
  note   = "Accessed Jan-2025"
}

@misc{nvidia-wiki,
  author = "{Wikipedia}",
  title  = {\href{https://en.wikipedia.org/wiki/Nvidia_DGX#Accelerators}
           {Nvidia DGX}},
  year   = "2025",
  note   = "Accessed Jan-2025"
}

%----------------------------------------------------------------------
% misc.
%----------------------------------------------------------------------
@article{parallel,
  title  = {\href{https://github.com/kingoflolz/mesh-transformer-jax}
           {{GPT-J-6B}: A 6 billion parameter autoregressive language model}},
  author = "Wang, Ben and Komatsuzaki, Aran",
  year   =  2021,
  note   = {\textit{Github repo}}
}

@article{gemma,
  title  = {\href{https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf}
           {Gemma: Open Models Based on Gemini Research and Technology}},
  author = "{Gemma Team, Google DeepMind}",
  year   =  2024,
}

@article{MoE,
  title  = {\href{https://huggingface.co/blog/moe}{Mixture of Experts Explained}},
  author = "Sanseviero, Omar and Tunstall, Lewis and Schmid, Philipp and
            Mangrulkar, Sourab and Belkada, Younes and Cuenca, Pedro",
  month  =  dec,
  year   =  2023,
  note   = {\textit{HuggingFace blog}}
}

@article{exp.c,
  title  = {\href{https://netlib.org/fdlibm/e_exp.c}{exp.c library}},
  author = "Sun Microsystems",
  year   =  2004,
  url    = {https://netlib.org/fdlibm/e_exp.c}
}

@article{tricks,
  title  = {\href{https://github.com/OpenMachine-ai/transformer-tricks}
           {Transformer tricks}},
  author = "OpenMachine",
  year   =  2024,
  url    = {https://github.com/OpenMachine-ai/transformer-tricks}
}

@article{hfFlashNorm,
  title  = {\href{https://huggingface.co/open-machine/FlashNorm}
           {FlashNorm}},
  author = "OpenMachine",
  year   =  2024,
  url    = {https://huggingface.co/open-machine/FlashNorm}
}

@misc{smollm,
  title  = {\href{https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B}
           {SmolLM2 - with great data, comes great performance}},
  author = {Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Martín Blázquez and Lewis Tunstall and Agustín Piqueres and Andres Marafioti and Cyril Zakka and Leandro von Werra and Thomas Wolf},
  year   = {2024},
  url    = {https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B}
}

@article{smolvlm,
  title  = {\href{https://huggingface.co/blog/smolvlm}
           {SmolVLM - small yet mighty Vision Language Model}},
  author = "Andres Marafioti and Merve Noyan and Miquel Farré and Elie Bakouch and Pedro Cuenca",
  month  =  nov,
  year   =  2024,
  note   = {\textit{HuggingFace blog}}
}

@article{gpt2,
  title  = {\href{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
           {Language Models are Unsupervised Multitask Learners}},
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year   = {2019}
}

@article{llava-next,
  title  = {\href{https://llava-vl.github.io/blog/2024-04-30-llava-next-video/}
           {LLaVA-NeXT: A Strong Zero-shot Video Understanding Model}},
  author = {Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},
  month  = {April},
  year={2024}
}

@article{vicuna,
  title  = {\href{https://lmsys.org/blog/2023-03-30-vicuna/}
           {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality}},
  author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
  month  = {March},
  year   = {2023}
}

@article{slim-video,
  title  = {\href{https://youtu.be/uVtk3B6YO4Y}
           {Podcast-video about Slim Attention}},
  author = "{Andrew Wasielewski with audio generated by Notebook LM}",
  month  = {Jan},
  year   = {2025}
}

@article{flashNorm-video,
  title  = {\href{https://youtu.be/GEuJv34_XgU?si}
           {Podcast-video about FlashNorm}},
  author = "{Christopher Marquand with audio generated by Notebook LM}",
  month  = {May},
  year   = {2025}
}

@article{llamafile,
  title  = {\href{https://github.com/Mozilla-Ocho/llamafile}
           {llamafile}},
  author = "Mozilla",
  url    = {https://github.com/Mozilla-Ocho/llamafile}
}

@article{vLLM,
  title  = {\href{https://github.com/vllm-project/vllm}
           {vLLM}},
  author = "vLLM Project",
  url    = {https://github.com/vllm-project/vllm}
}

@article{lmstudio,
  title  = {\href{https://lmstudio.ai}
           {LM Studio}},
  author = "LM Studio",
  url    = {https://lmstudio.ai}
}

@article{ollama,
  title  = {\href{https://github.com/ollama/ollama}
           {Ollama}},
  author = "Ollama",
  url    = {https://github.com/ollama/ollama}
}

@article{sglang,
  title  = {\href{https://github.com/sgl-project/sglang}
           {SGLang}},
  author = "SGLang",
  url    = {https://github.com/sgl-project/sglang}
}

@article{HFtransformers,
  title  = {\href{https://huggingface.co/docs/transformers}
           {Transformers}},
  author = "HuggingFace",
  url    = {https://huggingface.co/docs/transformers}
}

@article{llama-cpp,
  title  = {\href{https://github.com/ggml-org/llama.cpp}
           {llama.cpp}},
  author = "Georgi Gerganov",
  url    = {https://github.com/ggml-org/llama.cpp}
}

@article{whisper-cpp,
  title  = {\href{https://github.com/ggml-org/whisper.cpp}
           {whisper.cpp}},
  author = "Georgi Gerganov",
  url    = {https://github.com/ggml-org/whisper.cpp}
}

@article{evo,
  title = {\href{https://www.biorxiv.org/content/10.1101/2024.02.27.582234v1}
          {Sequence modeling and design from molecular to genome scale with Evo}},
  author = {Eric Nguyen and Michael Poli and Matthew G. Durrant and Brian Kang and Dhruva Katrekar and David B. Li and Liam J. Bartie and Armin W. Thomas and Samuel H. King and Garyk Brixi and Jeremy Sullivan and Madelena Y. Ng and Ashley Lewis and Aaron Lou and Stefano Ermon and Stephen A. Baccus and Tina Hernandez-Boussard and Christopher Ré and Patrick D. Hsu and Brian L. Hie },
  journal = {Science},
  volume = {386},
  number = {6723},
  year = {2024},
}

@article{slimAttn,
  title  = {\href{https://github.com/OpenMachine-ai/transformer-tricks/blob/main/doc/slimAttn.pdf}
           {Slim attention: cut your context memory in half without loss of accuracy -- \emph{K-cache is all you need for MHA}}},
  author = "Nils Graef and Andrew Wasielewski",
  year   = 2025,
  url    = {https://github.com/OpenMachine-ai/transformer-tricks/blob/main/doc/slimAttn.pdf}
}

@article{matShrink,
  title  = {\href{https://github.com/OpenMachine-ai/transformer-tricks/blob/main/doc/matShrink.pdf}
           {Matrix-shrink for transformers without loss of accuracy}},
  author = "Nils Graef",
  year   = 2025,
  url    = {https://github.com/OpenMachine-ai/transformer-tricks/blob/main/doc/matShrink.pdf}
}
