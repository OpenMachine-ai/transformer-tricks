{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccfc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proof of concept for the paper \"Slim Attention: cut your context memory in half\"\n",
    "# Usage: python3 slimAttn_example.py\n",
    "\n",
    "%pip install --quiet transformer_tricks\n",
    "import transformer_tricks as tt\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoConfig\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# defs\n",
    "#-------------------------------------------------------------------------------\n",
    "def softmax(x, axis=-1):\n",
    "  \"\"\"softmax along 'axis', default is the last axis\"\"\"\n",
    "  e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))  # subtract max for numerical stability\n",
    "  return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "def msplit(M, h):\n",
    "  \"\"\"shortcut to split matrix M into h chunks\"\"\"\n",
    "  return np.array_split(M, h, axis=-1)\n",
    "\n",
    "def ops(A, B):\n",
    "  \"\"\"number of ops for matmul of A and B;\n",
    "  -  A and B must be 2D arrays, and their inner dimensions must agree!\n",
    "   - A is an m × n matrix, and B is an n × p matrix, then the resulting product\n",
    "     of A and B is an m × p matrix.\n",
    "   - Each element (i,j) of the m x p result matrix is computed by the dot-product\n",
    "     of the i-th row of A and the j-th column of B.\n",
    "   - Each dot-product takes n multiplications and n - 1 additions, so total\n",
    "     number of ops is 2n - 1 per dotproduct.\n",
    "   - There are m * p elements in the result matrix, so we have m * p dotproducts,\n",
    "     so in total we need m * p * (2n - 1) ops, which is approximately 2*m*p*n ops\n",
    "   - For simplicity, let's just use the simple approximation of OPS = 2*m*p*n\"\"\"\n",
    "  m, n = A.shape\n",
    "  p = B.shape[1]\n",
    "  return 2 * m * n * p\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# setup for model SmolLM2-1.7B\n",
    "#-------------------------------------------------------------------------------\n",
    "tt.quiet_hf()  # calm down HuggingFace\n",
    "\n",
    "repo = 'HuggingFaceTB/SmolLM2-1.7B'\n",
    "param = tt.get_param(repo)\n",
    "config = AutoConfig.from_pretrained(repo)\n",
    "\n",
    "h = config.num_attention_heads\n",
    "d = config.hidden_size\n",
    "dk = config.head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# check if we can accurately compute V from K for each layer\n",
    "#-------------------------------------------------------------------------------\n",
    "for layer in range(config.num_hidden_layers):\n",
    "  # convert to float64 for better accuracy of matrix inversion\n",
    "  Wk = param[tt.weight('K', layer)].to(torch.float64).numpy()\n",
    "  Wv = param[tt.weight('V', layer)].to(torch.float64).numpy()\n",
    "  Wkv = np.linalg.inv(Wk) @ Wv\n",
    "  print(layer, ':', np.allclose(Wk @ Wkv, Wv))  # check if Wk @ Wkv is the 'same' as Wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab800a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# compare option 1 and option 2 for calculating equation (5) of paper\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# get weights for Q, K, V and convert to float64\n",
    "# TODO: some or all weights are actually transposed in the tensorfile!\n",
    "Wq = param[tt.weight('Q', 0)].to(torch.float64).numpy()\n",
    "Wk = param[tt.weight('K', 0)].to(torch.float64).numpy()\n",
    "Wv = param[tt.weight('V', 0)].to(torch.float64).numpy()\n",
    "Wkv = np.linalg.inv(Wk) @ Wv # calculate Wkv (aka W_KV)\n",
    "# print('Is Wk @ Wkv close to Wv?', np.allclose(Wk @ Wkv, Wv))\n",
    "\n",
    "# generate random input X\n",
    "n = 100  # number of tokens\n",
    "X = np.random.rand(n, d).astype(np.float64)  # range [0,1]\n",
    "Xn = np.expand_dims(X[n-1, :], axis=0)  # the n-th row of X, and make it a 1 x d matrix\n",
    "\n",
    "Q = Xn @ Wq  # only for the last row of X (for the generate-phase)\n",
    "K = X @ Wk\n",
    "V = X @ Wv\n",
    "\n",
    "# only consider the first head\n",
    "Q0, K0, V0 = msplit(Q, h)[0], msplit(K, h)[0], msplit(V, h)[0]\n",
    "Wkv0 = msplit(Wkv, h)[0]\n",
    "\n",
    "# baseline reference\n",
    "scores = softmax((Q0 @ K0.T) / np.sqrt(dk))\n",
    "head_ref = scores @ V0\n",
    "\n",
    "# head option1 and option2\n",
    "head_o1 = scores @ (K @ Wkv0)  # option 1\n",
    "head_o2 = (scores @ K) @ Wkv0  # option 2\n",
    "\n",
    "# compare\n",
    "print('Is head_o1 close to head_ref?', np.allclose(head_o1, head_ref))\n",
    "print('Is head_o2 close to head_ref?', np.allclose(head_o2, head_ref))\n",
    "\n",
    "# computational complexity for both options\n",
    "o1_step1, o1_step2 = ops(K, Wkv0), ops(scores, (K @ Wkv0))\n",
    "o2_step1, o2_step2 = ops(scores, K), ops(scores @ K, Wkv0)\n",
    "\n",
    "print(f'Option 1 OPS: step 1 = {o1_step1:,}; step 2 = {o1_step2:,}; total = {(o1_step1 + o1_step2):,}')\n",
    "print(f'Option 2 OPS: step 1 = {o2_step1:,}; step 2 = {o2_step2:,}; total = {(o2_step1 + o2_step2):,}')\n",
    "print(f'speedup of option 2 over option 1: {((o1_step1 + o1_step2) / (o2_step1 + o2_step2)):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289da2b8",
   "metadata": {},
   "source": [
    "Whenever you change this file, make sure to regenerate the jupyter notebook as follows:\n",
    "  `jupytext slimAttn_concept.py -o ../notebooks/slimAttn_concept.ipynb`"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
